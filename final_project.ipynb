{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tropical Cyclones Classification Project\n",
    "\n",
    "\n",
    "Link to the data: https://www.kaggle.com/datasets/noaa/hurricane-database/data\n",
    "\n",
    "About the data:\n",
    "\n",
    "The National Hurricane Center (NHC) conducts a post-storm analysis of each tropical cyclone in the Atlantic\n",
    "basin (i.e., North Atlantic Ocean, Gulf of Mexico, and Caribbean Sea) and the North Pacific Ocean to determine the official assessment of the cyclone's history. This analysis makes use of all available observations, including those that may not have been available in real time. In addition, NHC conducts ongoing reviews of any retrospective tropical cyclone analyses brought to its attention and on a regular basis updates the historical record to reflect\n",
    "changes introduced.\n",
    "\n",
    "Full description of the data can be found here: https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf\n",
    "\n",
    "Goal: Classify the cyclones detected in the atlantic ocean (i.e., determine the type of the cyclone, target class: 'Status')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a96dc649073097"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import learning_curve, validation_curve, train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from scipy.stats import loguniform, beta, uniform\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uploading the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4e89c24a5ce41"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic = pd.read_csv('data/atlantic_corrupted.csv')\n",
    "data_atlantic.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da65fa3e559f97ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe1a92d30f113ee",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset consists of over 49 thousand instances and 22 features. We shall now take a closer look to the dataset and do the first preprocessing step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b6dfdc4c700f4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Preprocessing\n",
    "\n",
    "Here, we must firstly deal with missing data, drop unnecessary features if needed and understand what kind of data we have."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a494f8e732df190a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74946e849a2cb8a2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we have is this:\n",
    "- 'Unnamed: 0' column is just an ID column generated by data_noiser code, we will remove it.\n",
    "- 'Event', 'Status', 'Latitude', 'Longitude' are all non numerical features. \n",
    "- 'ID' and 'Name' are the same thing, they just identify the certain event.\n",
    "\n",
    "We shall remove 'Unnamed: 0' column now, since it was generated by data noiser and just indicates the id of samples, which we have in any case."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e846fc976f5fc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic.set_index('Unnamed: 0', inplace=True)\n",
    "data_atlantic.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "790d091cef3c9b3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's take a closer look and evaluate how much data is missing for each column."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa740dc45d095aa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic.isnull().sum(axis=0) / data_atlantic.shape[0] * 100 # just for fun, let's see it in 'percentage' "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca6a9bc24105d859",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we see is this:\n",
    "- 'Time', 'Latitude', 'Maximum Wind', 'Low Wind NE', 'Low Wind SE', 'Moderate Wind NW' and 'High Wind SW' have moderately high percentage of missing data.\n",
    "- The rest of the features have the same percentage of missing data, so we 'suspect' there are samples full of only missing data. Let's check:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "363e47b83029f5e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# we check on axis=1, i.e. all the rows, for the instances full of only missing data by comparing the sum of missing values to the number of columns\n",
    "print(f'Amount of samples containing only missing data: {(data_atlantic.isnull().sum(axis=1) == data_atlantic.shape[1]).sum()}')\n",
    "print(f'The percentage of such rows: {((data_atlantic.isnull().sum(axis=1) == data_atlantic.shape[1]).sum())/data_atlantic.shape[0] * 100}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b0558a922fc833",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have nearly 500 rows with missing data. This is less than 1% of total data, we can remove these rows."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2765e20417a2ee8d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic.dropna(how='all', axis=0, inplace=True)\n",
    "data_atlantic.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b3b40c5dbeb1ee",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we go back to the rest of the missing data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ffe81e7a56ff233"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_atlantic.isnull().sum(axis=0) / data_atlantic.shape[0] * 100  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1fb29c36f3b98a9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Time has a float value here and missing values shall be substituted by median value.\n",
    "- Latitude and Longitude here are object, but it can be transformed into numerical features + missing values will be substitued by mean values.\n",
    "- Maximum Wind, Low Wind NE, Low Wind SE, Moderate Wind NW, High Wind SW, Moderate Wind SE are all numerical features, mean values will be imputed.\n",
    "\n",
    "Alright, we know what to do with missing values; what shall we do with the rest of the features (i.e., the ones with the type 'object')\n",
    "\n",
    "- ID and Name are practically the same, one of them will be dropped, the other substituted by integers.\n",
    "- Latitude and Longitude can be converted to float by using extra function we shall define below\n",
    "- Status is a categorical feature, and here we can just encode names of classes by mapping them to the integers.\n",
    "\n",
    "Before we proceed, we take closer look at the 'Status' and 'Event' features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97846a0638b5c6c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sns.countplot(x='Status', data=data_atlantic, palette='pastel')\n",
    "print(data_atlantic.Status.unique())\n",
    "print(len(data_atlantic.Status.unique()[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6c19d4926874fd9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Status can be the following types:\n",
    "- TD – Tropical cyclone of tropical depression intensity (< 34 knots)\n",
    "- TS – Tropical cyclone of tropical storm intensity (34-63 knots)\n",
    "- HU – Tropical cyclone of hurricane intensity (> 64 knots)\n",
    "- EX – Extratropical cyclone (of any intensity)\n",
    "- SD – Subtropical cyclone of subtropical depression intensity (< 34 knots)\n",
    "- SS – Subtropical cyclone of subtropical storm intensity (> 34 knots)\n",
    "- LO – A low that is neither a tropical cyclone, a subtropical cyclone, nor an extratropical cyclone (of any intensity)\n",
    "- WV – Tropical Wave (of any intensity)\n",
    "- DB – Disturbance (of any intensity)\n",
    "\n",
    "Status is our target class; we shall encode these categories and learn the model to classify the cyclones.\n",
    "\n",
    "As for the Event, if we take a closer look..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42a3d59393ffbc61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "L – Record identifier\n",
    "C – Closest approach to a coast, not followed by a landfall\n",
    "G – Genesis\n",
    "I – An intensity peak in terms of both pressure and wind\n",
    "L – Landfall (center of system crossing a coastline)\n",
    "P – Minimum in central pressure\n",
    "R – Provides additional detail on the intensity of the cyclone when rapid changes are underway\n",
    "S – Change of status of the system\n",
    "T – Provides additional detail on the track (position) of the cyclone\n",
    "W – Maximum sustained wind speed\n",
    "\n",
    "While these value carry some information, we can easily remove them.\n",
    "\n",
    "Thus, we shall apply the following transformations:\n",
    "\n",
    "1. Features representing data bout Wind, Pressure, Time: \n",
    "        - SimpleImputer, i.e. substitute missing values with mean of each column\n",
    "        - StandardScaler, i.e. bring features to the same scale \n",
    "2. Latitude, Longitude:\n",
    "        - CoordinateTransformer: define the new transformer class which takes these features and returns numerical values instead of strings\n",
    "        - StandardScaler (bring them to the same scale)\n",
    "3. Event:\n",
    "        - EventTransformer: define the new transformer class which transforms the categories into numerical features.\n",
    "4. Date:\n",
    "        - DateTransformer: define the new transformer to change the format of the dates in the dataset.\n",
    "\n",
    "\n",
    "The rest shall be dropped.\n",
    "\n",
    "### Defining extra Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a87ded01eaa72040"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CoordinateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    The class converts given coordinates into numerical values (float) \n",
    "    while preserving the information about the coordinate (N, E, W, S)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def coordinate_mapping(self, x):\n",
    "        # if latitude/longitude value has S/W at the end, we indicate it by multiplying the numerical value by -1\n",
    "        # otherwise, it's normal\n",
    "        coord = float(x[:-1]) # convert the string into float, except for the last symbol\n",
    "        if x[-1] in [\"W\", \"S\"]:\n",
    "            coord *= -1\n",
    "        return coord\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"Latitude\"] = X[\"Latitude\"].apply(self.coordinate_mapping)\n",
    "        X[\"Longitude\"] = X[\"Longitude\"].apply(self.coordinate_mapping)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a270eca3dca2ea8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EventTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping=None):\n",
    "        if mapping is None:\n",
    "            self.mapping = {\n",
    "                '  ': 0, ' L': 1, ' C': 2, ' G': 3, ' I': 4, ' R': 5, \n",
    "                ' P': 6, ' W': 7, ' S': 8, ' T': 9\n",
    "            }\n",
    "        else:\n",
    "            self.mapping = mapping\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X['Event'] = X['Event'].map(self.mapping)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c7046d7f2448fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class converts the format to '%Y%m%d' and then creates 3 new features for storing Year, Month, Day\n",
    "    \"\"\"\n",
    "    def __init__(self, date_format='%Y%m%d'):\n",
    "        self.date_format = date_format\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        date = pd.to_datetime(X['Date'], format=self.date_format)\n",
    "        X['Year'] = date.dt.year\n",
    "        X['Month'] = date.dt.month\n",
    "        X['Day'] = date.dt.day\n",
    "        X = X.drop(['Date'], axis=1)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c5f10fb528a3b5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create the pipelines and the final column transformer.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddd15e9bef390a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pipeline_event = Pipeline([\n",
    "    ('event_changer', EventTransformer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline_date = Pipeline([\n",
    "    ('change_dates', DateTransformer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline_winds_time = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline_coords = Pipeline([\n",
    "    ('transform', CoordinateTransformer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "final_transformer = ColumnTransformer([\n",
    "    ('categories', pipeline_event, ['Event']),\n",
    "    ('winds_of_change', pipeline_winds_time, ['Time', 'Maximum Wind', 'Minimum Pressure', 'Low Wind NE',\n",
    "       'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE',\n",
    "       'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW',\n",
    "       'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW']),\n",
    "    ('coords', pipeline_coords, ['Latitude', 'Longitude']),\n",
    "    ('dates', pipeline_date, ['Date']),\n",
    "    #('id_transformer', OneHotEncoder(categories='auto', drop='first', handle_unknown='ignore'), ['ID'])\n",
    "],\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=False,\n",
    "    sparse_threshold=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa16f77aed1a48ee",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train - Test Split\n",
    "\n",
    "Now we are splitting the data into train and test sets. \n",
    "As we said before, 'Status' is our target, y."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee51c3ca1730e65b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = data_atlantic.drop(['Status'], axis=1)\n",
    "y = data_atlantic['Status']\n",
    "y = y.map({' HU': 1, ' TS': 2, ' EX': 3, ' TD': 4, ' LO': 5, ' DB': 6, ' SD': 7, ' SS': 8, ' WV': 9})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=43, \n",
    "                                                    stratify=y, \n",
    "                                                    shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "348cbc6afa9a2440",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ce7032e2de3e434"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_pipeline = IMBPipeline([ # each element in a Pipeline is mainly a step related to a different technique \n",
    "    ('transformer', final_transformer),  # transformation\n",
    "    ('sampler', SMOTE()), # another transformer;\n",
    "    ('dim_reduction', PCA(n_components=0.8)), # dimensionality reduction, if we want\n",
    "    ('classifier', Perceptron()) # classifier, of course - there is a placeholder Perceptron, BUT it can be subsittuted with other model\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f158f004fe8e0fa7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train) # just a check that al the elements are correctly defined; can be omitted"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6df9451dcf3fa723",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# the reason hyperparameters are called like this is because without specification (sampler, dim_reduction, etc.) we may have the case of two hyperparameters with the same name in different 'boxes' and the program will crash\n",
    "\n",
    "sampler_configs = [\n",
    "    {\n",
    "        'sampler':[None],# The element is bypassed\n",
    "    },\n",
    "    {\n",
    "        'sampler':[RandomUnderSampler()],\n",
    "        'sampler__sampling_strategy':['majority', 1.2, 0.9, 0.7]\n",
    "    },\n",
    "    {\n",
    "        'sampler':[RandomOverSampler()],\n",
    "        'sampler__sampling_strategy':['minority', 1.2, 0.9, 0.7]\n",
    "    }\n",
    "]\n",
    "\n",
    "dim_reduction_configs = [\n",
    "    {\n",
    "        'dim_reduction': [None]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [PCA()],\n",
    "        'dim_reduction__n_components': [0.5, 0.7, 0.8]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [SFS(estimator=Perceptron(), cv = None, scoring = 'f1_weighted', forward=False)], # sequential Backward Selection\n",
    "        'dim_reduction__estimator': [LogisticRegression(random_state=42, solver='saga')],\n",
    "        'dim_reduction__k_features' : [5,7,10]  \n",
    "    }\n",
    "]\n",
    "\n",
    "classifier_configs = [\n",
    "    {\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [5, 7, 9, 11]\n",
    "    },\n",
    "    {\n",
    "        'classifier' : [RandomForestClassifier(bootstrap=True)],\n",
    "        'classifier__n_estimators' : [50, 100, 500, 700],\n",
    "        'classifier__max_depth': [4, 6, 8, 10],\n",
    "        'classifier__criterion': ['gini', 'entropy', 'log_loss']\n",
    "    },\n",
    "    {\n",
    "        'classifier': [OneVsRestClassifier(estimator=Perceptron())],\n",
    "        'classifier__estimator': [{\n",
    "            'classifier': LogisticRegression(solver='saga', random_state=42),\n",
    "            'classifier__C': loguniform(0.001, 100),\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "            'classifier__class_weight': ['balanced', None]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [XGBClassifier()],\n",
    "        'classifier__n_estimators' : [100, 250, 500, 1000],\n",
    "        'classifier__learning_rate' : [0.001, 0.05, 0.01, 0.1],\n",
    "        'classifier__max_depth' : [6, 8, 10]\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824d6cb7b3c0ed4d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_configs = [dict(itertools.chain(*(e.items() \n",
    "for e in configuration))) \n",
    "for configuration in \n",
    "itertools.product(sampler_configs,dim_reduction_configs,classifier_configs)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a36689457a9a62e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "f'Number of all possible configurations: {len(all_configs)}'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c985fcaaf591b133",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(model_pipeline,\n",
    "                        param_distributions=all_configs,\n",
    "                        n_iter=len(all_configs)*5,\n",
    "                        n_jobs=-1,\n",
    "                        cv=2, \n",
    "                        scoring='f1_weighted')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8212991c03c5d032",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scores = cross_validate(rs, X_train, y_train, scoring='f1_weighted', \n",
    "                        cv=5, \n",
    "                        return_estimator=True, \n",
    "                        verbose=3) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc3b88e9be5d7692",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for index, estimator in enumerate(scores['estimator']):\n",
    "    print(estimator.best_estimator_.get_params()['sampler'])\n",
    "    print(estimator.best_estimator_.get_params()['dim_reduction'])\n",
    "    print(estimator.best_estimator_.get_params()['classifier'],estimator.best_estimator_.get_params()['classifier'].get_params())\n",
    "    print(scores['test_score'][index])\n",
    "    print('-'*10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a2eeb8398faad4c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, for now the best model is this one: \n",
    "None\n",
    "SequentialFeatureSelector(cv=None, estimator=RandomForestClassifier(),\n",
    "                          forward=False, k_features=(10, 10),\n",
    "                          scoring='f1_weighted')\n",
    "RandomForestClassifier(criterion='entropy', max_depth=10) {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7db367497ce343a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for estimator in scores['estimator']:\n",
    "    pred_train = estimator.best_estimator_.fit(X_train, y_train)\n",
    "    pred_train = estimator.best_estimator_.predict(X_train)\n",
    "    pred_test = estimator.best_estimator_.predict(X_test)\n",
    "    f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "    f1_test = f1_score(y_test, pred_test, average='weighted')\n",
    "    print(f'Estimator:{estimator.best_estimator_.get_params()['classifier']}, F1 on training set:{f1_train}, F1 on test set:{f1_test}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30b894d573d97635",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first model performs better on the test set than all the others, so we make a final pick: Estimator:RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=500)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5aa89d39df7b48e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Refinement of the Selected Model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebc75cccd146a72c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_model_pipeline = IMBPipeline([\n",
    "    ('trans', final_transformer),\n",
    "    ('classifier',RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=500))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'classifier__max_depth': [8, 10, 12], \n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6be4a572654ef807",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rs_best = RandomizedSearchCV(\n",
    "    estimator = best_model_pipeline,\n",
    "    param_distributions=params,\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3),\n",
    "    n_iter=10,\n",
    "    scoring='f1_weighted'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bab91baf31d1c1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rs_best.fit(X_train, y_train) # 20 min"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ff403f118f23b09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rs_best.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85484736949c2b60",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "f1_score(y_test, rs_best.best_estimator_.predict(X_test), average='weighted')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "714e8adacb729220",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = rs_best.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5149ba0af999fcf8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(cls,\n",
    "                                                        X=X_train,\n",
    "                                                        y=y_train,\n",
    "                                                        train_sizes=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                                        cv=5, \n",
    "                                                        n_jobs=-1,\n",
    "                                                        scoring='f1_weighted',\n",
    "                                                        shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e925ee1a4cfd99",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='+',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='d', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('Learning curve')\n",
    "ax.set_ylim([0.60, 1.03])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf99c0e2f7ca772b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "range_max_depth = [8, 10, 12, 14]\n",
    "train_scores, test_scores = validation_curve(cls,\n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        param_range=\n",
    "        range_max_depth, \n",
    "        param_name='classifier__max_depth',\n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        scoring='f1_weighted'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c0f826f23608b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(range_max_depth, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "ax.fill_between(range_max_depth,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(range_max_depth, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "ax.fill_between(range_max_depth,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.set_xlabel('Max_depth')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.8, 1.0])\n",
    "ax.set_xlim([6, 15])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f248a8e0cc99b56",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
